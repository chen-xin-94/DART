{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = Path(\"..\")\n",
    "\n",
    "# setup paths\n",
    "dataset_dir = Path(\"/mnt/ssd2/xin/repo/DART/Liebherr_Product\")\n",
    "\n",
    "# Define the images directory and duplicates directory using Path objects\n",
    "image_dir = dataset_dir / \"images\"\n",
    "meta_dir = dataset_dir / \"metadata\"\n",
    "label_dir = dataset_dir / \"labels\"\n",
    "yolo_dir = dataset_dir / \"yolo\"\n",
    "yolo_labels_dir = yolo_dir / \"labels\"\n",
    "yolo_labels_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "# List all objects in the image directory\n",
    "objs = sorted([obj.name for obj in image_dir.iterdir()])\n",
    "\n",
    "with open(meta_dir / \"id_to_name.json\", \"r\") as f:\n",
    "    id_to_name = json.load(f)\n",
    "\n",
    "with open(meta_dir / \"near_duplicates.json\", \"r\") as f:\n",
    "    near_duplicates = json.load(f)\n",
    "\n",
    "with open(label_dir / \"labels_nms.json\", \"r\") as f:\n",
    "    labels_nms = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(label_dir / \"no_gpt.json\", \"r\") as f:\n",
    "    no_gpt = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  articulated dump truck: 0\n",
      "  bulldozer: 1\n",
      "  combined piling and drilling rig: 2\n",
      "  crawler crane: 3\n",
      "  crawler excavator: 4\n",
      "  crawler loader: 5\n",
      "  duty cycle crane: 6\n",
      "  gantry crane: 7\n",
      "  log loader: 8\n",
      "  maritime crane: 9\n",
      "  material handling machine: 10\n",
      "  mining bulldozer: 11\n",
      "  mining excavator: 12\n",
      "  mining truck: 13\n",
      "  mobile crane: 14\n",
      "  pipelayer: 15\n",
      "  pontoon excavator: 16\n",
      "  reachstacker: 17\n",
      "  telescopic handler: 18\n",
      "  tower crane: 19\n",
      "  truck mixer: 20\n",
      "  wheel excavator: 21\n",
      "  wheel loader: 22\n"
     ]
    }
   ],
   "source": [
    "with open(meta_dir / \"classes.json\", \"r\") as f:\n",
    "    class_dict = json.load(f)\n",
    "\n",
    "for k, v in class_dict.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_not_approved(X_train, X_val, X_test, no_gpt):\n",
    "    \"\"\"\n",
    "    delete files not approved by gpt from the list no_gpt\n",
    "    \"\"\"\n",
    "    X_train_array = np.array(X_train)\n",
    "    X_val_array = np.array(X_val)\n",
    "    X_test_array = np.array(X_test)\n",
    "    no_gpt_array = np.array(no_gpt)\n",
    "    mask_train = np.isin(X_train_array, no_gpt_array)\n",
    "    mask_val = np.isin(X_val_array, no_gpt_array)\n",
    "    mask_test = np.isin(X_test_array, no_gpt_array)\n",
    "\n",
    "    X_train_gpt = X_train_array[~mask_train].tolist()\n",
    "    X_val_gpt = X_val_array[~mask_val].tolist()\n",
    "    X_test_gpt = X_test_array[~mask_test].tolist()\n",
    "\n",
    "    return X_train_gpt, X_val_gpt, X_test_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(\n",
    "    image_list,\n",
    "    y_list,\n",
    "    no_list,\n",
    "    train_ratio=0.6,\n",
    "    val_ratio=0.2,\n",
    "    test_ratio=0.2,\n",
    "    seed=SEED,\n",
    "):\n",
    "    # Convert lists to numpy arrays for easy indexing\n",
    "    image_array = np.array(image_list)\n",
    "    y_array = np.array(y_list)\n",
    "\n",
    "    if no_list is None:\n",
    "        remaining_images = image_array\n",
    "        remaining_labels = y_array\n",
    "        special_images = np.array([])\n",
    "        special_labels = np.array([])\n",
    "    else:\n",
    "        # Identify indices of special images\n",
    "        no_indices = np.isin(image_array, no_list)\n",
    "\n",
    "        # Separate special images and their labels\n",
    "        special_images = image_array[no_indices]\n",
    "        special_labels = y_array[no_indices]\n",
    "\n",
    "        # Remaining images and their labels\n",
    "        remaining_images = image_array[~no_indices]\n",
    "        remaining_labels = y_array[~no_indices]\n",
    "\n",
    "    # Calculate the number of samples in each set\n",
    "    total_samples = len(image_array)\n",
    "    num_special_images = len(special_images)\n",
    "    num_remaining_samples = total_samples - num_special_images\n",
    "\n",
    "    # Adjust train size to include the special images\n",
    "    adjusted_train_ratio = train_ratio - (num_special_images / total_samples)\n",
    "    train_size = int(adjusted_train_ratio * num_remaining_samples)\n",
    "    # test_size = int(test_ratio * total_samples)\n",
    "    # val_size = num_remaining_samples - train_size - test_size\n",
    "    val_size = int(val_ratio * num_remaining_samples)\n",
    "    test_size = int(test_ratio * num_remaining_samples)\n",
    "\n",
    "    # Split the remaining data into train, val, and test sets in a stratified manner\n",
    "    # # method 1\n",
    "    # X_train, X_temp, y_train, y_temp = train_test_split(remaining_images, remaining_labels, test_size=test_size+val_size, stratify=remaining_labels, random_state=seed)\n",
    "    # X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_size, stratify=y_temp, random_state=seed)\n",
    "    # # method 2\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        remaining_images,\n",
    "        remaining_labels,\n",
    "        test_size=test_size,\n",
    "        stratify=remaining_labels,\n",
    "        random_state=seed,\n",
    "    )\n",
    "    if val_ratio == 0:\n",
    "        X_train = X_temp\n",
    "        y_train = y_temp\n",
    "        X_val = np.empty(0)\n",
    "        y_val = np.empty(0)\n",
    "    else:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_size, stratify=y_temp, random_state=seed\n",
    "        )\n",
    "\n",
    "    # Combine the train set with the special images\n",
    "    X_train = np.concatenate((X_train, special_images))\n",
    "    y_train = np.concatenate((y_train, special_labels))\n",
    "\n",
    "    return (\n",
    "        X_train.tolist(),\n",
    "        X_val.tolist(),\n",
    "        X_test.tolist(),\n",
    "        y_train.tolist(),\n",
    "        y_val.tolist(),\n",
    "        y_test.tolist(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "near_duplicates_list = [f[:5] for f in near_duplicates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "id_to_y = {}\n",
    "for id, ann in labels_nms.items():\n",
    "    if ann[\"boxes\"] == []:  # Skip images with no annotations\n",
    "        continue\n",
    "    obj = id_to_name[id + \".jpg\"].split(\"/\")[0]\n",
    "    ids.append(id)\n",
    "    id_to_y[id] = class_dict[obj]\n",
    "ys = [id_to_y[id] for id in ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80/20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stratified, dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9878 2384 3008\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = stratified_split(\n",
    "    ids, ys, None, 0.64, 0.16, 0.2\n",
    ")\n",
    "\n",
    "## if in near_duplicates_list move to X_train\n",
    "for id in X_val:\n",
    "    if id in near_duplicates_list:\n",
    "        X_train.append(id)\n",
    "        X_val.remove(id)\n",
    "for id in X_test:\n",
    "    if id in near_duplicates_list:\n",
    "        X_train.append(id)\n",
    "        X_test.remove(id)\n",
    "\n",
    "print(len(X_train), len(X_val), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = [\n",
    "    f\"./images/{id_to_name[id+'.jpg'].split('/')[0]}/{id}.jpg\" for id in X_train\n",
    "]\n",
    "val_list = [f\"./images/{id_to_name[id+'.jpg'].split('/')[0]}/{id}.jpg\" for id in X_val]\n",
    "test_list = [\n",
    "    f\"./images/{id_to_name[id+'.jpg'].split('/')[0]}/{id}.jpg\" for id in X_test\n",
    "]\n",
    "\n",
    "train_file = yolo_dir / \"train.txt\"\n",
    "val_file = yolo_dir / \"val.txt\"\n",
    "trainval_file = yolo_dir / \"trainval.txt\"\n",
    "test_file = yolo_dir / \"test.txt\"\n",
    "\n",
    "\n",
    "with open(train_file, \"w\") as f:\n",
    "    for item in train_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(val_file, \"w\") as f:\n",
    "    for item in val_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(trainval_file, \"w\") as f:\n",
    "    for item in train_list + val_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(test_file, \"w\") as f:\n",
    "    for item in test_list:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gpt guided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = stratified_split(\n",
    "    ids, ys, None, 0.64, 0.16, 0.2\n",
    ")\n",
    "X_train_gpt, X_val_gpt, X_test_gpt = delete_not_approved(X_train, X_val, X_test, no_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = [\n",
    "    f\"./images/{id_to_name[id+'.jpg'].split('/')[0]}/{id}.jpg\" for id in X_train_gpt\n",
    "]\n",
    "val_list = [\n",
    "    f\"./images/{id_to_name[id+'.jpg'].split('/')[0]}/{id}.jpg\" for id in X_val_gpt\n",
    "]\n",
    "test_list = [\n",
    "    f\"./images/{id_to_name[id+'.jpg'].split('/')[0]}/{id}.jpg\" for id in X_test_gpt\n",
    "]\n",
    "\n",
    "train_file = yolo_dir / \"train_gpt.txt\"\n",
    "val_file = yolo_dir / \"val_gpt.txt\"\n",
    "trainval_file = yolo_dir / \"trainval_gpt.txt\"\n",
    "test_file = yolo_dir / \"test_gpt.txt\"\n",
    "\n",
    "\n",
    "with open(train_file, \"w\") as f:\n",
    "    for item in train_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(val_file, \"w\") as f:\n",
    "    for item in val_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(trainval_file, \"w\") as f:\n",
    "    for item in train_list + val_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(test_file, \"w\") as f:\n",
    "    for item in test_list:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
